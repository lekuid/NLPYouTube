{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0e8482-cfe6-41b7-a09a-fb159bfa65f6",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import os, glob, random, time, ffmpy\n",
    "import speech_recognition as sr\n",
    "import wave, contextlib\n",
    "from google.cloud import speech\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0686ef9-f22a-4c3c-86cf-bae0c8e61d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34101be6-b93b-48ca-a96f-75ced2675668",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total pages scraped: 1\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('G:\\My Drive\\Desktop\\Tools\\chromedriver89.exe');\n",
    "driver.get('https://www.youtube.com/c/TED/videos'); time.sleep(3);\n",
    "#driver.get('https://www.youtube.com/c/TED/videos?view=0&sort=da&flow=grid'); time.sleep(3)\n",
    "\n",
    "page = []\n",
    "\n",
    "while len(page)<1:\n",
    "    driver.find_element_by_css_selector(\"div#search-input > input#search\").send_keys(Keys.PAGE_DOWN);\n",
    "    page.append(driver.find_elements_by_xpath(\"/html//div[@id='details']/div[@id='meta']//a[@href]\"));\n",
    "    clear_output()\n",
    "    print(f'Total pages scraped: {len(page)}');\n",
    "for x in page[-1]:\n",
    "    if x.get_attribute('href') not in links:\n",
    "        links.append(x.get_attribute('href'))\n",
    "    else:\n",
    "        pass\n",
    "driver.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7ce00c-ebf8-4836-ba6d-e5b69accab5e",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             0\n",
       "0  https://www.youtube.com/watch?v=Qm02X0aE8uU\n",
       "1  https://www.youtube.com/watch?v=Axh07mJ9Ag4\n",
       "2  https://www.youtube.com/watch?v=Mkelhs_OVMc\n",
       "3  https://www.youtube.com/watch?v=g4xGbbDACDw\n",
       "4  https://www.youtube.com/watch?v=CpzjPnDW1G4"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.youtube.com/watch?v=Qm02X0aE8uU</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.youtube.com/watch?v=Axh07mJ9Ag4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.youtube.com/watch?v=Mkelhs_OVMc</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.youtube.com/watch?v=g4xGbbDACDw</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.youtube.com/watch?v=CpzjPnDW1G4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "pd.DataFrame(links).to_csv(f'TEDx_links.csv', index = False)\n",
    "data = pd.read_csv(f'TEDx_links.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08dcf18e-fdb4-496e-bf50-3d59cc9204a2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://www.youtube.com/watch?v=Qm02X0aE8uU'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "str(data.loc[0]).split(' ')[4].split('\\n')[0] \n",
    "#This feels too long just to query the value at index as string instead of object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f7d7f4-4f8a-44ab-8923-26be10a73ac4",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total links: 30\n"
     ]
    }
   ],
   "source": [
    "print(f'Total links: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloaded 3 videos.\n",
      "All videos converted.\n"
     ]
    }
   ],
   "source": [
    "#Remove old files to make a clean batch\n",
    "for file in [x for x in os.listdir('rawFiles')]:\n",
    "    os.remove(f'rawFiles\\\\{file}')\n",
    "\n",
    "#Download videos from YouTube using scraped link list\n",
    "for x in range(3):\n",
    "    yt = YouTube(str(data.loc[x]).split(' ')[4].split('\\n')[0])\n",
    "    yt.streams.filter(only_audio=True)[0].download('rawFiles')\n",
    "    clear_output()\n",
    "    print(f'Downloaded {x+1} videos.')\n",
    "\n",
    "#Convert .mp4 to .wav and encoding filename for processing\n",
    "for idx, file in enumerate([x for x in os.listdir('rawFiles') if x.split('.')[-1]=='mp4']):\n",
    "    ff = ffmpy.FFmpeg(executable=\"drivers\\\\ffmpeg.exe\",\n",
    "                      inputs = {f\"rawFiles\\\\{file}\" : None},\n",
    "                      outputs = {f\"rawFiles\\\\{idx}.wav\" : f'-acodec pcm_s16le -ac 1 -ar 16000'})\n",
    "    ff.run()\n",
    "    os.remove(f'rawFiles\\\\{file}')\n",
    "print('All videos converted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73579261-0186-4e00-9432-422026c261e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Method1: Splitting by a fixed time length.\n",
    "for file in [x for x in os.listdir('splitFiles')]:\n",
    "    os.remove(f'splitFiles\\\\{file}')\n",
    "\n",
    "cut_len = 10 #seconds\n",
    "\n",
    "for idx, file in enumerate([x for x in os.listdir('rawFiles')]):\n",
    "    with contextlib.closing(wave.open(f'rawFiles\\\\{file}','r')) as f: #opening the file to read\n",
    "        duration = round(f.getnframes() / float(f.getframerate())) #total frames / the framerate\n",
    "    x=0\n",
    "    while x < (duration/cut_len): #doing it this way to have x as 1 or could have simply added cut_len to x at each iter\n",
    "        ff = ffmpy.FFmpeg(executable=\"drivers\\\\ffmpeg.exe\",\\\n",
    "                          inputs = {f\"rawFiles\\\\{file}\" : None},\\\n",
    "                          outputs = {f\"splitFiles\\\\{idx}_{x}.wav\" : f'-ss {x*cut_len} -t {cut_len}'}) # -ss(starting at) -t(seconds to keep) that 12 is to crop out TEDx intro sound :P\n",
    "        ff.run()\n",
    "        x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'for file in [x for x in os.listdir(\\'rawFiles\\') if x.split(\\'.\\')[-1]==\\'wav\\']:\\n    song = AudioSegment.from_wav(f\"rawFiles\\\\{file}\")\\n    chunks = split_on_silence(song, min_silence_len = 800, silence_thresh = -50)\\n    for idx, chunk in enumerate(chunks):\\n        chunk.export(f\"splitFiles\\\\{file}_{idx}.wav\", format=\\'wav\\')'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#Method2: Splitting by silence in audio.\n",
    "\n",
    "'''for file in [x for x in os.listdir('rawFiles') if x.split('.')[-1]=='wav']:\n",
    "    song = AudioSegment.from_wav(f\"rawFiles\\\\{file}\")\n",
    "    chunks = split_on_silence(song, min_silence_len = 800, silence_thresh = -50)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.export(f\"splitFiles\\\\{file}_{idx}.wav\", format='wav')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19b7f7ed-c077-45f1-ac98-c3f4e5495688",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = [x for x in os.listdir('splitFiles')]\n",
    "\n",
    "transcript = []\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "for file in files:\n",
    "    temp=[]\n",
    "    audio = sr.AudioFile(f'splitFiles\\\\{file}')\n",
    "    with audio as source:\n",
    "        audio_file = r.record(source)\n",
    "        try:\n",
    "            temp = (r.recognize_google(audio_file))\n",
    "            transcript.append(temp)\n",
    "            pd.DataFrame(transcript).to_csv(f'TEDx_Lines.csv', index = False)\n",
    "        except:\n",
    "            pass\n",
    "print(len(transcript))\n",
    "print(transcript[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "f16ab138-03dd-42f2-ae3c-04f327efd16d",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"of 55 countries is getting easier and faster by the day so it won't be long before none effect\""
      ]
     },
     "metadata": {},
     "execution_count": 757
    }
   ],
   "source": [
    "transcript[random.randint(0, len(transcript))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "4717e2ec-b1bf-4bc7-99dd-545d99edc40b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ng': 715, 'roshan': 716, 'perspectives': 717, 'microbes': 718, 'assemble': 719, 'interdisciplinary': 720, 'teams': 721, 'shot': 722, 'chasing': 723, 'famous': 724, 'collaborated': 725, 'engineers': 726, 'robots': 727, 'gentle': 728, 'stressors': 729, 'jellyfish': 730, 'radcliffe': 731, 'institute': 732, 'spicy': 733, 'innova': 734, 'crysta': 735, 'ise': 736, 'papaya': 737, 'cents': 738, 'universe': 739, 'songs': 740, 'future': 741, 'began': 742, 'discussing': 743, 'facebook': 744, 'anytime': 745, 'profibus': 746, 'tracking': 747, 'intraspecies': 748, 'code': 749, \"didn't\": 750, 'sindhi': 751, 'lada': 752, 'report': 753, 'analysing': 754, 'communicate': 755, 'sequences': 756, 'regional': 757, 'dialects': 758, 'british': 759, 'powerful': 760, 'analyse': 761, 'map': 762, 'meaning': 763, 'scientists': 764, 'unknown': 765, 'stone': 766, 'dimensional': 767, 'updated': 768, 'researchers': 769, 'selected': 770, 'annotated': 771, 'thousand': 772, 'sperm': 773, 'boneta': 774, 'millions': 775, 'annotate': 776, 'cognitive': 777, 'behaviour': 778, 'arial': 779, 'grounds': 780, 'race': 781, 'outsource': 782, 'partners': 783, 'dominicus': 784, '226': 785, '25': 786, 'known': 787, 'families': 788, 'interactions': 789, 'sulphur': 790, 'audio': 791, 'contacts': 792, 'admission': 793, 'west': 794, 'indies': 795, '1st': 796, 'odi': 797, 'syntax': 798, 'displacement': 799, 'shown': 800, 'carefully': 801, 'simplest': 802, 'encouraging': 803, 'global': 804, 'intelligent': 805, 'cortex': 806, 'spindle': 807, 'cell': 808, 'skin': 809, 'emotions': 810, 'memory': 811, '8000': 812, 'party': 813, 'raja': 814, 'pain': 815, 'carried': 816, 'sing': 817, 'spot': 818, 'whales': 819, 'movement': 820, 'led': 821, 'prevent': 822, 'approval': 823, 'extinction': 824, 'slaughter': 825, 'dialogue': 826, 'titli': 827, 'show': 828, 'magical': 829, 'respect': 830, 'tend': 831, 'sowcarpet': 832, 'services': 833, 'asian': 834, 'consider': 835, 'purchasing': 836, 'loyalty': 837, 'influences': 838, 'marketing': 839, 'bringing': 840, 'advise': 841, 'individuals': 842, 'investing': 843, 'footprints': 844, 'sole': 845, 'organisation': 846, 'completely': 847, 'miss': 848, 'consumer': 849, 'case': 850, \"you're\": 851, 'wondering': 852, 'shame': 853, 'misconceptions': 854, 'win': 855, 'wallets': 856, 'office': 857, '1': 858, 'billion': 859, 'potential': 860, 'figure': 861, 'dipping': 862, 'myths': 863, 'misunderstandings': 864, 'citizens': 865, 'ajeeb': 866, 'maybe': 867, 'legacy': 868, 'poverty': 869, 'famine': 870, 'forex': 871, 'push': 872, 'dangerous': 873, 'assumption': 874, 'option': 875, 'synostose': 876, 'parts': 877, 'buying': 878, 'fats': 879, 'according': 880, '80': 881, 'deeply': 882, 'decisions': 883, 'moscow': 884, 'economies': 885, 'usually': 886, 'durability': 887, 'functionality': 888, 'efficiency': 889, 'non': 890, 'aftertaste': 891, 'nutrition': 892, 'forest': 893, 'economic': 894, 'nation': 895, 'myself': 896, 'connected': 897, 'rest': 898, 'travel': 899, 'differences': 900, 'seasonality': 901, 'clothes': 902, 'pushed': 903, 'europe': 904, 'yrs': 905, 'membership': 906, 'vessels': 907, 'sub': 908, 'pushing': 909, 'specifically': 910, 'fit': 911, 'desire': 912, 'budgets': 913, 'milk': 914, \"shouldn't\": 915, 'pushpit': 916, 'fabric': 917, 'lines': 918, 'highly': 919, 'conscious': 920, 'loyal': 921, 'longest': 922, 'gonna': 923, 'associated': 924, 'bit': 925, 'inherited': 926, 'ran': 927, 'preferences': 928, 'crime': 929, 'cosmetics': 930, 'east': 931, 'measures': 932, 'introduced': 933, 'princes': 934, 'older': 935, 'financially': 936, 'daughters': 937, 'reach': 938, 'recognise': 939, 'cooking': 940, 'colleagues': 941, 'hazard': 942, 'visit': 943, 'nearest': 944, 'cook': 945, 'visitor': 946, 'orange': 947, 'stoney': 948, 'receipt': 949, 'packed': 950, 'soft': 951, 'drink': 952, 'coke': 953, 'toothpaste': 954, 'colgate': 955, 'margarine': 956, 'ram': 957, 'endless': 958, 'believe': 959, 'present': 960, 'places': 961, 'param': 962, 'misconception': 963, 'emerging': 964, 'leggings': 965, 'innovation': 966, 'technological': 967, 'frock': 968, 'smith': 969, 'leap': 970, 'frog': 971, 'kids': 972, 'stops': 973, 'lives': 974, 'fox': 975, 'goes': 976, 'beyond': 977, 'massive': 978, 'saturation': 979, 'due': 980, 'leapfrogs': 981, 'close': 982, '60': 983, 'gdp': 984, 'kenya': 985, 'months': 986, 'apartment': 987, 'move': 988, 'asked': 989, \"month's\": 990, 'rent': 991, 'deposit': 992, 'sorry': 993, 'honestly': 994, 'someone': 995, 'weak': 996, 'transformation': 997, 'e': 998, 'commerce': 999, 'agreement': 1000, 'transactions': 1001, 'port': 1002, 'crossing': 1003, 'borders': 1004, 'piece': 1005, 'action': 1006, 'pains': 1007, '2021': 1008, 'length': 1009, 'summer': 1010, 'common': 1011, 'individual': 1012, 'h': 1013, '2000': 1014, 'therefore': 1015, 'positioning': 1016, 'segments': 1017, 'vary': 1018, 'substantially': 1019, 'weeds': 1020, 'tea': 1021, 'spyware': 1022, 'pegasus': 1023, '33': 1024, 'traditional': 1025, 'kenyans': 1026, 'prefer': 1027, 'supermarket': 1028, 'tokens': 1029, 'look': 1030, 'ten': 1031, 'girlfriends': 1032, 'paper': 1033, 'methods': 1034, 'females': 1035, 'grew': 1036, 'township': 1037, 'prasuta': 1038, 'employment': 1039, 'studied': 1040, 'members': 1041, 'appreciated': 1042, 'afflict': 1043, 'tribes': 1044, 'tastes': 1045, 'heavily': 1046, 'entrench': 1047, 'influencers': 1048, 'thrive': 1049, 'youth': 1050, 'edition': 1051, 'plans': 1052, 'levels': 1053, 'essay': 1054, 'businesses': 1055, 'booming': 1056, 'desirable': 1057, 'photo': 1058, 'intentionally': 1059, 'ek': 1060, 'uncle': 1061, 'ka': 1062, 'bali': 1063, 'linguist': 1064, 'profession': 1065, 'academia': 1066, 'startup': 1067, 'national': 1068, 'sport': 1069, 'building': 1070, 'dream': 1071, 'land': 1072, 'labs': 1073, 'field': 1074, 'atal': 1075, 'worry': 1076, 'board': 1077, 'artificial': 1078, 'intelligence': 1079, 'birthday': 1080, 'buzzword': 1081, 'everybody': 1082, 'nlp': 1083, 'simple': 1084, 'engineering': 1085, 'generate': 1086, 'interacting': 1087, 'bath': 1088, 'train': 1089, 'based': 1090, 'underpins': 1091, 'allergy': 1092, 'basic': 1093, 'huge': 1094, 'set': 1095, 'algorithm': 1096, 'patterns': 1097, 'password': 1098, 'deep': 1099, 'neural': 1100, 'networks': 1101, 'underpin': 1102, 'peace': 1103, 'details': 1104, 'week': 1105, 'humongous': 1106, 'converse': 1107, 'gujarat': 1108, 'gujarati': 1109, '17': 1110, 'recognition': 1111, 'transcribe': 1112, 'better': 1113, 'transcribed': 1114, '18': 1115, 'bse': 1116, 'nse': 1117, 'fiction': 1118, 'true': 1119, 'front': 1120, 'eyes': 1121, 'giant': 1122, 'beats': 1123, 'technical': 1124, 'tweets': 1125, 'limited': 1126, 'friend': 1127, 'mine': 1128, 'kuli': 1129, 'detail': 1130, 'says': 1131, 'four': 1132, 'arabic': 1133, 'spanish': 1134, 'kedarnath': 1135, 'handful': 1136, '90': 1137, 'kapil': 1138, 'bypassed': 1139, '5000': 1140, 'researches': 1141, 'technologist': 1142, 'attracted': 1143, 'towards': 1144, 'webel': 1145, 'rich': 1146, 'richer': 1147, 'cycle': 1148, 'resourceful': 1149, 'nobody': 1150, 'implication': 1151, 'pending': 1152, 'scap': 1153, 'innovative': 1154, 'thermotech': 1155, 'weeks': 1156, 'applications': 1157, 'ceo': 1158, 'theoretically': 1159, 'quotation': 1160, 'concrete': 1161, 'exam': 1162, 'driven': 1163, 'states': 1164, 'norwegian': 1165, 'robust': 1166, 'tribals': 1167, 'unesco': 1168, 'atlas': 1169, 'danger': 1170, 'designated': 1171, 'ngo': 1172, 'provides': 1173, 'journalism': 1174, 'portal': 1175, 'gold': 1176, 'phones': 1177, 'absolutely': 1178, 'support': 1179, 'created': 1180, 'moderated': 1181, 'manually': 1182, \"we're\": 1183, 'brought': 1184, 'antioxidants': 1185, 'academic': 1186, 'institution': 1187, 'profit': 1188, 'publisher': 1189, 'kasam': 1190, 'importantly': 1191, 'themselves': 1192, 'participated': 1193, 'activity': 1194, 'children': 1195, 'aadivasi': 1196, 'articles': 1197, 'provided': 1198, 'cg': 1199, 'net': 1200, 'sara': 1201, 'users': 1202, 'gone': 1203, 'down': 1204, 'replicate': 1205, 'guess': 1206, 'owning': 1207, 'total': 1208, 'tools': 1209, 'granted': 1210, 'vivek': 1211, 'msr': 1212, 'elaborate': 1213, 'manoj': 1214, 'chopra': 1215, 'providing': 1216, 'microtask': 1217, 'underserved': 1218, 'provide': 1219, 'signified': 1220, 'labour': 1221, 'rural': 1222, 'program': 1223, 'everyday': 1224, 'la': 1225, 'literate': 1226, 'wants': 1227, 'no2': 1228, 'oota': 1229, 'amla': 1230, 'wardha': 1231, 'district': 1232, 'maharashtra': 1233, 'decided': 1234, 'sure': 1235, 'lots': 1236, 'audience': 1237, 'definitely': 1238, 'remote': 1239, 'tv': 1240, 'electricity': 1241, 'signal': 1242, 'climb': 1243, 'helen': 1244, 'wave': 1245, 'lessons': 1246, 'sprite': 1247, \"one's\": 1248, 'army': 1249, 'thrilled': 1250, 'advancing': 1251, 'effort': 1252, 'importance': 1253, 'telling': 1254, 'source': 1255, 'daytime': 1256, 'wadood': 1257, 'recordings': 1258, 'retail': 1259, 'recount': 1260, 'scientist': 1261, 'caught': 1262, 'increase': 1263, 'accuracy': 1264, 'happy': 1265, 'keeps': 1266, 'upfront': 1267, 'start': 1268, 'policy': 1269, 'interventions': 1270, 'sometime': 1271, 'allowed': 1272, 'farmers': 1273, 'agricultural': 1274, 'videos': 1275, 'tum': 1276, 'bhi': 1277, 'madhya': 1278, 'pradesh': 1279, 'covered': 1280, 'results': 1281, 'wb': 1282, 'confused': 1283, 'happening': 1284, 'booked': 1285, 'discovered': 1286, 'collected': 1287, 'silent': 1288, 'quiet': 1289, \"hadn't\": 1290, 'heard': 1291, 'constant': 1292, 'insects': 1293, 'recording': 1294, 'insect': 1295, 'distorting': 1296, 'colleague': 1297, 'rani': 1298, 'meri': 1299, 'searcher': 1300, 'pronounce': 1301, 'sanskrit': 1302, 'ionic': 1303, 'padarthon': 1304, 'kharpatwar': 1305, 'niyantran': 1306, 'chemical': 1307, 'pesticides': 1308, 'farming': 1309, 'interact': 1310, 'dawai': 1311, 'kida': 1312, 'maarna': 1313, 'medicine': 1314, 'learnt': 1315, 'hope': 1316, 'majority': 1317, 'investment': 1318, 'creation': 1319, 'unlikely': 1320, 'efficient': 1321, 'manner': 1322, 'extremely': 1323, 'derives': 1324, 'whatever': 1325, 'deliver': 1326, 'positive': 1327, 'modified': 1328, 'methodology': 1329, '4d': 1330, 'develop': 1331, 'problem': 1332, 'solve': 1333, 'particular': 1334, 'observation': 1335, 'allocate': 1336, 'needed': 1337, 'diversity': 1338, 'properties': 1339, 'adapted': 1340, 'aur': 1341, 'frequently': 1342, 'later': 1343, 'lead': 1344, 'pursue': 1345, 'aboriginal': 1346, 'patricia': 1347, \"o'connor\": 1348, 'isabella': 1349, 'ola': 1350, 'mid': 1351, '97': 1352, 'university': 1353, 'queensland': 1354, 'wanted': 1355, 'bluntly': 1356, 'cannot': 1357, 'memories': 1358, 'traditions': 1359, 'literature': 1360, 'founded': 1361, 'bombay': 1362, 'museum': 1363, 'page': 1364, 'written': 1365, 'sami': 1366, 'finland': 1367, 'canada': 1368, 'mundari': 1369, 'wagon': 1370, 'hubble': 1371, 'telescope': 1372, 'construct': 1373, 'station': 1374, 'ironically': 1375, 'higona': 1376, 'assigned': 1377, 'rescue': 1378, 'craft': 1379, 'timer': 1380, 'projects': 1381, 'ideas': 1382, 'watching': 1383, 'lunch': 1384, 'enjoyed': 1385, 'walked': 1386, 'roof': 1387, 'lining': 1388, 'gangster': 1389, 'countdown': 1390, 's': 1391, 'racketeering': 1392, 'pad': 1393, 'outpouring': 1394, 'emotion': 1395, 'crying': 1396, 'device': 1397, 'assessment': 1398, 'took': 1399, 'accomplished': 1400, 'astronauts': 1401, 'force': 1402, 'engineer': 1403, 'cheap': 1404, 'watched': 1405, 'twice': 1406, 'terrifying': 1407, 'standing': 1408, 'rooftop': 1409, 'noida': 1410, 'dancewear': 1411, 'helplessness': 1412, \"i've\": 1413, 'seen': 1414, 'operation': 1415, 'import': 1416, 'house': 1417, 'infant': 1418, 'seat': 1419, 'cockpit': 1420, 'behind': 1421, 'wallpaper': 1422, 'marching': 1423, 'interspace': 1424, 'returned': 1425, 'safely': 1426, 'preparations': 1427, 'healthy': 1428, 'running': 1429, 'mummy': 1430, 'abe': 1431, 'namami': 1432, 'daddy': 1433, 'oven': 1434, 'forth': 1435, 'video': 1436, 'conference': 1437, 'dance': 1438, 'creating': 1439, 'message': 1440, 'caring': 1441, 'mum': 1442, 'dad': 1443, 'forward': 1444, 'reader': 1445, 'questioned': 1446, 'flying': 1447, 'pilot': 1448, 'serial': 1449, 'answer': 1450, 'foremost': 1451, 'walaikum': 1452, 'star': 1453, 'prospective': 1454, 'changed': 1455, 'nasa': 1456, 'self': 1457, 'exploration': 1458, 'discovery': 1459, 'cage': 1460, 'engaged': 1461, 'texas': 1462, 'away': 1463, 'meg': 1464, 'challenger': 1465, 'tragically': 1466, 'confronted': 1467, 'reality': 1468, 'uske': 1469, 'brave': 1470, 'press': 1471, 'balance': 1472, 'knowing': 1473, 'risk': 1474, 'ave': 1475, 'accomplish': 1476, 'participating': 1477, 'trainer': 1478, 'literally': 1479, 'hours': 1480, 'preparing': 1481, 'inserted': 1482, 'bite': 1483, 'size': 1484, 'pieces': 1485, 'instructor': 1486, 'professional': 1487, 'instructors': 1488, 'exact': 1489, 'yourself': 1490, 'singer': 1491, 'unnecessarily': 1492, 'difference': 1493, 'supporting': 1494, 'whole': 1495, 'account': 1496, 'fare': 1497, 'couple': 1498, 'director': 1499, 'reluctant': 1500, 'observed': 1501, 'foreign': 1502, 'usse': 1503, 'invited': 1504, 'mention': 1505, 'tab': 1506, 'singers': 1507, 'fine': 1508, 'changes': 1509, 'personality': 1510, 'officer': 1511, 'beginning': 1512, 'launched': 1513, 'busy': 1514, 'pest': 1515, 'features': 1516, 'programs': 1517, 'brief': 1518, 'exercise': 1519, 'scene': 1520, 'observe': 1521, 'earth': 1522, 'saw': 1523, 'storm': 1524, 'colours': 1525, 'lights': 1526, 'cloud': 1527, 'door': 1528, 'dramatic': 1529, 'laser': 1530, 'patient': 1531, 'struck': 1532, 'folder': 1533, 'blackness': 1534, 'incredibly': 1535, 'thin': 1536, 'layer': 1537, 'keeping': 1538, 'wearing': 1539, 'strong': 1540, 'urge': 1541, 'several': 1542, 'attached': 1543, 'stars': 1544, 'elastic': 1545, 'discharge': 1546, 'godspeed': 1547, 'mechanic': 1548, 'order': 1549, 'pastor': 1550, 'wonders': 1551, 'slides': 1552, 'prints': 1553, 'oh': 1554, 'god': 1555, 'signifies': 1556, 'patti': 1557, 'actual': 1558, 'monosyllabic': 1559, 'weekend': 1560, 'extend': 1561, 'direction': 1562, 'path': 1563, 'turn': 1564, 'breakfast': 1565, 'chalice': 1566, 'shareit': 1567, 'moving': 1568, 'le': 1569, 'sakte': 1570, 'nastiest': 1571, 'bolna': 1572, 'pole': 1573, 'contention': 1574, 'obvious': 1575, 'heavyweight': 1576, 'contenders': 1577, 'piano': 1578, 'both': 1579, 'brutalisation': 1580, 'genocide': 1581, 'hire': 1582, 'applied': 1583, 'humanizing': 1584, 'saying': 1585, 'surely': 1586, 'basil': 1587, 'plant': 1588, 'festival': 1589, 'wing': 1590, 'oldest': 1591, 'davies': 1592, 'germany': 1593, 'vikings': 1594, 'leading': 1595, 'series': 1596, 'shared': 1597, 'proto': 1598, 'chain': 1599, 'ac': 1600, '6': 1601, 'jain': 1602, 'theory': 1603, 'ecology': 1604, 'cunning': 1605, 'naked': 1606, 'sumsung': 1607, 'boob': 1608, 'uninstall': 1609, 'counselling': 1610, 'formulas': 1611, 'hybrid': 1612, 'middle': 1613, 'ages': 1614, 'quint': 1615, 'latin': 1616, 'variations': 1617, 'tense': 1618, 'including': 1619, '50': 1620, 'ride': 1621, 'empress': 1622, 'heavy': 1623, 'cotton': 1624, 'holdings': 1625, 'palatable': 1626, 'attend': 1627, 'steps': 1628, '17th': 1629, 'disorder': 1630, 'interview': 1631, 'friendship': 1632, 'derived': 1633, 'hot': 1634, 'since': 1635, 'wisdom': 1636, 'queen': 1637, 'stopping': 1638, 'recognising': 1639, 'p': 1640, 'covid': 1641, '19': 1642, 'ladies': 1643, 'bling': 1644, 'pic': 1645, 'calling': 1646, 'considering': 1647, 'ends': 1648, 'haryana': 1649, 'clock': 1650, '1066': 1651, '1219': 1652, 'consultant': 1653, '446': 1654, '2': 1655, 'white': 1656, 'khan': 1657, 'turns': 1658, 'norfolk': 1659, 'stitchery': 1660, 'aliases': 1661, 'arrested': 1662, 'dahej': 1663, 'gives': 1664, 'fabulous': 1665, 'dairy': 1666, 'farm': 1667, '12925': 1668, 'tomorrow': 1669, 'offence': 1670, 'picture': 1671, 'lcd': 1672, 'descriptive': 1673, 'examples': 1674, 'proverbs': 1675, 'depending': 1676, '1325': 1677, 'advice': 1678, '15': 1679, 'cm': 1680, 'favourite': 1681, 'celebrity': 1682, 'simply': 1683, 'secreting': 1684, 'surprises': 1685, 'treatment': 1686, 'celebrated': 1687, 'tower': 1688, 'chastity': 1689, 'pal': 1690, 'taureans': 1691, 'liberated': 1692, 'latest': 1693, 'plastomers': 1694, 'bond': 1695, 'felt': 1696, 'wings': 1697, 'fly': 1698, 'syllabus': 1699, 'crop': 1700, 'infosys': 1701, 'awful': 1702, 'points': 1703, 'court': 1704, 'client': 1705, 'service': 1706, 'caused': 1707, 'current': 1708, 'winter': 1709, 'innocent': 1710, '20': 1711, 'mayor': 1712, 'ham': 1713, '8': 1714, 'freezing': 1715, 'supply': 1716, 'lap': 1717, 'lord': 1718, 'searching': 1719, 'matches': 1720, 'flutter': 1721, 'drive': 1722, 'please': 1723, 'simultaneous': 1724, 'austin': 1725, 'swept': 1726, 'rug': 1727, '1787': 1728, 'thomas': 1729, 'paul': 1730, 'shakespeare': 1731, 'jug': 1732, 'surprise': 1733, 'libelous': 1734, 'situations': 1735, 'pamphlet': 1736, 'terrorism': 1737, 'welding': 1738, 'conjunction': 1739, 'depressed': 1740, 'violence': 1741, 'stream': 1742, 'sexual': 1743, 'liberation': 1744, 'tok': 1745, 'worth': 1746, 'user': 1747, 'considered': 1748, 'fence': 1749, 'restoration': 1750, 'modern': 1751, 'chat': 1752, 'hair': 1753, 'rochester': 1754, 'absolute': 1755, 'pm': 1756, 'players': 1757, 'broke': 1758, 'divinity': 1759, 'leaves': 1760, 'contains': 1761, 'seed': 1762, 'drama': 1763, 'food': 1764, 'digested': 1765, 'sirf': 1766, 'mail': 1767, 'following': 1768, 'drawn': 1769, 'bats': 1770, 'overnight': 1771, 'oats': 1772, 'noti': 1773, 'georgian': 1774, 'gave': 1775, 'bailey': 1776, 'exploits': 1777, 'whipping': 1778, 'cream': 1779, 'sweden': 1780, 'refurbished': 1781, 'cheating': 1782, '1785': 1783, 'sisters': 1784, 'slang': 1785, 'appointment': 1786, 'nasty': 1787, 'stitching': 1788, 'starch': 1789, 'mrs': 1790, 'probes': 1791, 'expertise': 1792, 'selling': 1793, 'sites': 1794, 'address': 1795, 'prices': 1796, 'descriptions': 1797, 'container': 1798, '17488': 1799, 'johny': 1800, 'cleveland': 1801, 'famously': 1802, 'busted': 1803, 'writing': 1804, 'breed': 1805, 'birth': 1806, 'extended': 1807, 'expressions': 1808, 'masi': 1809, 'loans': 1810, 'khand': 1811, 'petition': 1812, 'predesh': 1813, 'victoria': 1814, 'hath': 1815, 'free': 1816, 'visual': 1817, 'literary': 1818, 'magazine': 1819, '7918': 1820, 'nahin': 1821, '2010': 1822, 'post': 1823, 'installed': 1824, 'vacant': 1825, \"isn't\": 1826, 'property': 1827, 'collective': 1828, 'noun': 1829, 'collector': 1830, 'charming': 1831, 'myth': 1832, 'student': 1833, 'victorians': 1834, 'writes': 1835, 'leiston': 1836, 'publication': 1837, 'subsequent': 1838, 'senator': 1839, \"chatterley's\": 1840, 'lover': 1841, '44': 1842, 'run': 1843, 'knots': 1844, 'graphics': 1845, 'shankar': 1846, 'familiar': 1847, 'constantly': 1848, 'woman': 1849, 'inspection': 1850, 'sunday': 1851, 'matter': 1852, 'entitled': 1853, 'section': 1854, '11th': 1855, 'spider': 1856, 'stuff': 1857, 'falcon': 1858, 'st': 1859, 'claret': 1860, 'pictures': 1861, 'never': 1862, 'ship': 1863, '1971': 1864, 'jack': 1865, 'nicholson': 1866, 'sona': 1867, 'button': 1868, '1996': 1869, 'v': 1870, 'climbing': 1871, 'finally': 1872, 'admitted': 1873, 'paragraph': 1874, 'seventies': 1875, '2014': 1876, 'released': 1877, 'sleeve': 1878, 'welfare': 1879, 'fall': 1880, 'grace': 1881, 'elite': 1882, 'i20': 1883, 'tandem': 1884, 'vagina': 1885, 'violent': 1886, '29': 1887, '30': 1888, 'driver': 1889, 'migraine': 1890, 'intermediate': 1891, 'proud': 1892, 'unpleasant': 1893, 'horrible': 1894, 'general': 1895, 'protecting': 1896, 'thakor': 1897, 'bacon': 1898, 'summary': 1899, 'debated': 1900, 'extractor': 1901, 'except': 1902, 'alternatives': 1903, 'generally': 1904, 'increased': 1905, 'sanitize': 1906, 'totally': 1907, 'ben': 1908, 'person': 1909, 'braxton': 1910, 'playing': 1911, 'covering': 1912, 'letter': 1913, 'plastic': 1914, 'brimming': 1915, 'kanha': 1916, 'diya': 1917, 'pelvic': 1918, 'floor': 1919, 'respawn': 1920, 'uncomfortable': 1921, 'age': 1922, 'baby': 1923, 'twinkle': 1924, 'anal': 1925, 'verge': 1926, 'britain': 1927, 'ancient': 1928, 'untested': 1929, 'origin': 1930}\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow tokenizer to create tokens of sentences by encoding words into int categoricals\n",
    "tokens = Tokenizer(num_words = 100)\n",
    "tokens.fit_on_texts(transcript)\n",
    "print(tokens.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd061a919983dc477d17668f0590c4ef435aa61ba471860ff92cef845a6c326a65b",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}